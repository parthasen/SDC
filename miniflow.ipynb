{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.slns2wyws\n",
    "    \n",
    "http://cs231n.stanford.edu/vecDerivs.pdf\n",
    "\n",
    "playground.tensorflow.org/#activation=relu&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=5&seed=0.97298&showTestData=false&discretize=false&percTrainData=50&x=false&y=false&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&batchSize_hide=true&noise_hide=true&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=true&dataset_hide=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You need to change the Add() class below.\n",
    "\"\"\"\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, inbound_neurons=[]):\n",
    "        # Neurons from which this Node receives values\n",
    "        self.inbound_neurons = inbound_neurons\n",
    "        # Neurons to which this Node passes values\n",
    "        self.outbound_neurons = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_neurons:\n",
    "            n.outbound_neurons.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_neurons` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Neuron):\n",
    "    def __init__(self):\n",
    "        # an Input neuron has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Neuron.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node where the value\n",
    "    # is passed as an argument to forward().\n",
    "    #\n",
    "    # All other neuron implementations should get the value\n",
    "    # of the previous neurons from self.inbound_neurons\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_neurons[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Add(Neuron):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_neurons[0] (`x`) and self.inbound_neurons[1] (`y`)\n",
    "        Neuron.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this neuron (`self.value`) to the sum of it's inbound_nodes.\n",
    "        \n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        #self.sum=sum(self.inbound_neurons[0],self.inbound_neurons[1])\n",
    "        #self.value=self.inbound_neurons[0].value+self.inbound_neurons[1].value\n",
    "        x_value = self.inbound_neurons[0].value\n",
    "        y_value = self.inbound_neurons[1].value\n",
    "        self.value = x_value + y_value\n",
    "\n",
    "\"\"\"\n",
    "No need to change anything below here!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_neurons = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    neurons = [n for n in input_neurons]\n",
    "    while len(neurons) > 0:\n",
    "        n = neurons.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_neurons:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            neurons.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_neurons)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_neurons:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_neuron, sorted_neurons):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted neurons.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_neuron`: A neuron in the graph, should be the output neuron (have no outgoing edges).\n",
    "        `sorted_neurons`: a topologically sorted list of neurons.\n",
    "\n",
    "    Returns the output neuron's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_neurons:\n",
    "        n.forward()\n",
    "\n",
    "    return output_neuron.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#above code can be as miniflow.py and then use below\n",
    "#from miniflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5 = 15 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "x, y = Input(), Input()\n",
    "\n",
    "f = Add(x, y)\n",
    "\n",
    "feed_dict = {x: 10, y: 5}\n",
    "\n",
    "sorted_neurons = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_neurons)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` neurons we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bonus Challenge!\n",
    "\n",
    "Write your code in Add (scroll down).\n",
    "\"\"\"\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, inbound_neurons=[], label=''):\n",
    "        # An optional description of the neuron - most useful for outputs.\n",
    "        self.label = label\n",
    "        # Neurons from which this Node receives values\n",
    "        self.inbound_neurons = inbound_neurons\n",
    "        # Neurons to which this Node passes values\n",
    "        self.outbound_neurons = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_neurons:\n",
    "            n.outbound_neurons.append(self)\n",
    "\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_neurons` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backward propagation.\n",
    "\n",
    "        Compute the gradient of the current Neuron with respect\n",
    "        to the input neurons. The gradient of the loss with respect\n",
    "        to the current Neuron should already be computed in the `gradients`\n",
    "        attribute of the output neurons.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "class Input(Neuron):\n",
    "    def __init__(self):\n",
    "        # An Input Neuron has no inbound neurons,\n",
    "        # so no need to pass anything to the Neuron instantiator\n",
    "        Neuron.__init__(self)\n",
    "\n",
    "    # NOTE: Input Neuron is the only Neuron where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other Neuron implementations should get the value\n",
    "    # of the previous neurons from self.inbound_neurons\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_neurons[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value:\n",
    "            self.value = value\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input Neuron has no inputs so we refer to ourself\n",
    "        # for the gradient\n",
    "        self.gradients = {self: 0}\n",
    "        for n in self.outbound_neurons:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Can you augment the Add class so that it accepts\n",
    "any number of neurons as input?\n",
    "\n",
    "Hint: this may be useful:\n",
    "https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n",
    "\"\"\"\n",
    "class Add(Neuron):\n",
    "    # You may need to change this...\n",
    "    def __init__(self, *inputs):\n",
    "        Neuron.__init__(self, inputs)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        For reference, here's the old way from the last\n",
    "        quiz. You'll want to write code here.\n",
    "        \"\"\"\n",
    "        x_value = self.inbound_neurons[0].value\n",
    "        y_value = self.inbound_neurons[1].value\n",
    "        z_value = self.inbound_neurons[2].value\n",
    "        self.value = x_value + y_value + z_value\n",
    "        \n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the neurons in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Neuron and the value is the respective value feed to that Neuron.\n",
    "\n",
    "    Returns a list of sorted neurons.\n",
    "    \"\"\"\n",
    "\n",
    "    input_neurons = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    neurons = [n for n in input_neurons]\n",
    "    while len(neurons) > 0:\n",
    "        n = neurons.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_neurons:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            neurons.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_neurons)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_neurons:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_Neuron, sorted_neurons):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted neurons.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_Neuron`: A Neuron in the graph, should be the output Neuron (have no outgoing edges).\n",
    "        `sorted_neurons`: a topologically sorted list of neurons.\n",
    "\n",
    "    Returns the output Neuron's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_neurons:\n",
    "        n.forward()\n",
    "\n",
    "    return output_Neuron.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 + 5 + 10 = 19 (according to miniflow)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "No need to change anything here! \n",
    "\n",
    "If all goes well, this should work after you\n",
    "modify the Add class in miniflow.py.\n",
    "\"\"\"\n",
    "\n",
    "from miniflow import *\n",
    "\n",
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "f = Add(x, y, z)\n",
    "\n",
    "feed_dict = {x: 4, y: 5, z: 10}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "# should output 19\n",
    "print(\"{} + {} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], feed_dict[z], output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write the Linear#forward method below!\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, inbound_neurons=[]):\n",
    "        # Neurons from which this Node receives values\n",
    "        self.inbound_neurons = inbound_neurons\n",
    "        # Neurons to which this Node passes values\n",
    "        self.outbound_neurons = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_neurons:\n",
    "            n.outbound_neurons.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_neurons` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Neuron):\n",
    "    def __init__(self):\n",
    "        # An Input Neuron has no inbound neurons,\n",
    "        # so no need to pass anything to the Neuron instantiator\n",
    "        Neuron.__init__(self)\n",
    "\n",
    "        # NOTE: Input Neuron is the only Neuron where the value\n",
    "        # may be passed as an argument to forward().\n",
    "        #\n",
    "        # All other Neuron implementations should get the value\n",
    "        # of the previous neurons from self.inbound_neurons\n",
    "        #\n",
    "        # Example:\n",
    "        # val0 = self.inbound_neurons[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Linear(Neuron):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Neuron.__init__(self, inputs)\n",
    "        \n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other neurons.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective neurons.\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "        \n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        self.value = self.bias.value\n",
    "        for w, x in zip(self.weights, self.inbound_neurons):\n",
    "            self.value += w.value * x.value\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the neurons in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Neuron and the value is the respective value feed to that Neuron.\n",
    "\n",
    "    Returns a list of sorted neurons.\n",
    "    \"\"\"\n",
    "\n",
    "    input_neurons = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    neurons = [n for n in input_neurons]\n",
    "    while len(neurons) > 0:\n",
    "        n = neurons.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_neurons:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            neurons.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_neurons)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_neurons:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_Neuron, sorted_neurons):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted neurons.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_Neuron`: A Neuron in the graph, should be the output Neuron (have no outgoing edges).\n",
    "        `sorted_neurons`: a topologically sorted list of neurons.\n",
    "\n",
    "    Returns the output Neuron's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_neurons:\n",
    "        n.forward()\n",
    "\n",
    "    return output_Neuron.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "#from miniflow import *\n",
    "\n",
    "x, y, z = Input(), Input(), Input()\n",
    "inputs = [x, y, z]\n",
    "\n",
    "weight_x, weight_y, weight_z = Input(), Input(), Input()\n",
    "weights = [weight_x, weight_y, weight_z]\n",
    "\n",
    "bias = Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "\tx: 6,\n",
    "\ty: 14,\n",
    "\tz: 3,\n",
    "\tweight_x: 0.5,\n",
    "\tweight_y: 0.25,\n",
    "\tweight_z: 1.4,\n",
    "\tbias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer introduced. Many neurons in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modify Linear#forward so that it linearly transforms\n",
    "input matrices, weights matrices and a bias vector to\n",
    "an output.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, inbound_layers=[]):\n",
    "        self.inbound_layers = inbound_layers\n",
    "        self.value = None\n",
    "        self.outbound_layers = []\n",
    "        for layer in inbound_layers:\n",
    "            layer.outbound_layers.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    While it may be strange to consider an input a layer when\n",
    "    an input is only an individual node in a layer, for the sake\n",
    "    of simpler code we'll still use Layer as the base class.\n",
    "    \n",
    "    Think of Input as collating many individual input nodes into\n",
    "    a Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # An Input layer has no inbound layers,\n",
    "        # so no need to pass anything to the Layer instantiator\n",
    "        Layer.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, inbound_layer, weights, bias):\n",
    "        # Notice the ordering of the input layers passed to the\n",
    "        # Layer constructor.\n",
    "        Layer.__init__(self, [inbound_layer, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this layer to the linear transform output.\n",
    "        \n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        inputs = self.inbound_layers[0].value\n",
    "        weights = self.inbound_layers[1].value\n",
    "        bias = self.inbound_layers[2].value\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the layers in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Layer and the value is the respective value feed to that Layer.\n",
    "\n",
    "    Returns a list of sorted layers.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    layers = [n for n in input_layers]\n",
    "    while len(layers) > 0:\n",
    "        n = layers.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_layer, sorted_layers):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Layers.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_layer`: A Layer in the graph, should be the output layer (have no outgoing edges).\n",
    "        `sorted_layers`: a topologically sorted list of layers.\n",
    "\n",
    "    Returns the output layer's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_layers:\n",
    "        n.forward()\n",
    "\n",
    "    return output_layer.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This scripts demonstrates how the new MiniFlow works!\n",
    "\n",
    "Update the Linear class in miniflow.py to work with\n",
    "numpy vectors (arrays) and matrices.\n",
    "\n",
    "Test your code here!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "x = np.array([[-1., -2.], [-1, -2]])\n",
    "w = np.array([[2., -3], [2., -3]])\n",
    "b = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {inputs: x, weights: w, bias: b}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fix the Sigmoid class so that it computes the sigmoid function\n",
    "on the forward pass!\n",
    "\n",
    "Scroll down to get started.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, inbound_layers=[]):\n",
    "        self.inbound_layers = inbound_layers\n",
    "        self.value = None\n",
    "        self.outbound_layers = []\n",
    "        for layer in inbound_layers:\n",
    "            layer.outbound_layers.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "    def __init__(self):\n",
    "        # An Input layer has no inbound layers,\n",
    "        # so no need to pass anything to the Layer instantiator\n",
    "        Layer.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input Layer has no inputs so we refer to ourself\n",
    "        # for the gradient\n",
    "        self.gradients = {self: 0}\n",
    "        for n in self.outbound_Layers:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, inbound_layer, weights, bias):\n",
    "        # Notice the ordering of the input layers passed to the\n",
    "        # Layer constructor.\n",
    "        Layer.__init__(self, [inbound_layer, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inbound_layers[0].value\n",
    "        weights = self.inbound_layers[1].value\n",
    "        bias = self.inbound_layers[2].value\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer):\n",
    "        Layer.__init__(self, [layer])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "        \n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        return 1./(1.+np.exp(-x))\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this layer to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "        \n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_layers[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the layers in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Layer and the value is the respective value feed to that Layer.\n",
    "\n",
    "    Returns a list of sorted layers.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    layers = [n for n in input_layers]\n",
    "    while len(layers) > 0:\n",
    "        n = layers.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_layer, sorted_layers):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Layers.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_layer`: A Layer in the graph, should be the output layer (have no outgoing edges).\n",
    "        `sorted_layers`: a topologically sorted list of layers.\n",
    "\n",
    "    Returns the output layer's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_layers:\n",
    "        n.forward()\n",
    "\n",
    "    return output_layer.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This network feeds the output of a linear transform\n",
    "to the sigmoid function.\n",
    "\n",
    "Finish implementing the Sigmoid class in miniflow.py!\n",
    "\n",
    "Feel free to play around with this network, too!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "x = np.array([[-1., -2.], [-1, -2]])\n",
    "w = np.array([[2., -3], [2., -3]])\n",
    "b = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {inputs: x, weights: w, bias: b}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COST==MSE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for layers in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_layers`: A list of layers with edges into this layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_layers=[]):\n",
    "        \"\"\"\n",
    "        Layer's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all layers need.\n",
    "        \"\"\"\n",
    "        # A list of layers with edges into this layer.\n",
    "        self.inbound_layers = inbound_layers\n",
    "        # The eventual value of this layer. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of layers that this layer outputs to.\n",
    "        self.outbound_layers = []\n",
    "        # Sets this layer as an outbound layer for all of\n",
    "        # this layer's inputs.\n",
    "        for layer in inbound_layers:\n",
    "            layer.outbound_layers.append(self)\n",
    "\n",
    "    def forward():\n",
    "        \"\"\"\n",
    "        Every layer that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Layer.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Layer) constructor. Weights and bias\n",
    "        # are treated like inbound layers.\n",
    "        Layer.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_layers[0].value\n",
    "        W = self.inbound_layers[1].value\n",
    "        b = self.inbound_layers[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer):\n",
    "        # The base class constructor.\n",
    "        Layer.__init__(self, [layer])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_layers[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "class MSE(Layer):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last layer for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Layer.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_layers[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_layers[1].value.reshape(-1, 1)\n",
    "        m = self.inbound_layers[0].value.shape[0]#total number of examples if we use (1 /m) * np.sum(diff**2)\n",
    "\n",
    "        diff = y - a\n",
    "        self.value = np.mean(diff**2)\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the layers in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Layer and the value is the respective value feed to that Layer.\n",
    "\n",
    "    Returns a list of sorted layers.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    layers = [n for n in input_layers]\n",
    "    while len(layers) > 0:\n",
    "        n = layers.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Layers.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test your MSE method with this script!\n",
    "\n",
    "No changes necessary, but feel free to play\n",
    "with this script to test your network.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a) \n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "15.BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for layers in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_layers`: A list of layers with edges into this layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_layers=[]):\n",
    "        \"\"\"\n",
    "        Layer's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all layers need.\n",
    "        \"\"\"\n",
    "        # A list of layers with edges into this layer.\n",
    "        self.inbound_layers = inbound_layers\n",
    "        # The eventual value of this layer. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of layers that this layer outputs to.\n",
    "        self.outbound_layers = []\n",
    "        # New property! Keys are the inputs to this layer and\n",
    "        # their values are the partials of this layer with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this layer as an outbound layer for all of\n",
    "        # this layer's inputs.\n",
    "        for layer in inbound_layers:\n",
    "            layer.outbound_layers.append(self)\n",
    "\n",
    "    def forward():\n",
    "        \"\"\"\n",
    "        Every layer that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward():\n",
    "        \"\"\"\n",
    "        Every layer that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Layer.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input layer has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost * 1\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Layer) constructor. Weights and bias\n",
    "        # are treated like inbound layers.\n",
    "        Layer.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_layers[0].value\n",
    "        W = self.inbound_layers[1].value\n",
    "        b = self.inbound_layers[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_layers.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_layers:\n",
    "            # Get the partial of the cost with respect to this layer.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this layer's inputs.\n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, self.inbound_layers[1].value.T)\n",
    "            # Set the partial of the loss with respect to this layer's weights.\n",
    "            self.gradients[self.inbound_layers[1]] += np.dot(self.inbound_layers[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this layer's bias.\n",
    "            self.gradients[self.inbound_layers[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer):\n",
    "        # The base class constructor.\n",
    "        Layer.__init__(self, [layer])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_layers[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        \n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_layers:\n",
    "            # Get the partial of the cost with respect to this layer.\n",
    "            grad_cost = n.gradients[self]\n",
    "            \"\"\"\n",
    "            TODO: Your code goes here!\n",
    "            \n",
    "            Set the gradients property to the gradients with respect to each input.\n",
    "            \n",
    "            NOTE: See the Linear layer and MSE layer for examples.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "class MSE(Layer):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last layer for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Layer.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_layers[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_layers[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_layers[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \n",
    "        This is the final layer of the network so outbound layers\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_layers[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_layers[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the layers in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Layer and the value is the respective value feed to that Layer.\n",
    "\n",
    "    Returns a list of sorted layers.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    layers = [n for n in input_layers]\n",
    "    while len(layers) > 0:\n",
    "        n = layers.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Layers.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.,  0.],\n",
      "       [ 0.,  0.]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[ 0.],\n",
      "       [ 0.]]), array([ 0.])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test your network here!\n",
    "\n",
    "No need to change this code, but feel free to tweak it\n",
    "to test your network!\n",
    "\n",
    "Make your changes to backward method of the Sigmoid class in miniflow.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is an approximation of Gradient Descent, the more batches processed by the neural network, the better the approximation.\n",
    "\n",
    "    Randomly sample a batch of data from the total dataset.\n",
    "    Running the network forward and backward to calculate the gradient (with data from (1)).\n",
    "    Apply the gradient descent update.\n",
    "    Repeat steps 1-3 until convergence or the loop is stopped by another mechanism (i.e. the number of epochs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gradient descent update equation=x=xcost/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for layers in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_layers`: A list of layers with edges into this layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_layers=[]):\n",
    "        \"\"\"\n",
    "        Layer's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all layers need.\n",
    "        \"\"\"\n",
    "        # A list of layers with edges into this layer.\n",
    "        self.inbound_layers = inbound_layers\n",
    "        # The eventual value of this layer. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of layers that this layer outputs to.\n",
    "        self.outbound_layers = []\n",
    "        # New property! Keys are the inputs to this layer and\n",
    "        # their values are the partials of this layer with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this layer as an outbound layer for all of\n",
    "        # this layer's inputs.\n",
    "        for layer in inbound_layers:\n",
    "            layer.outbound_layers.append(self)\n",
    "\n",
    "    def forward():\n",
    "        \"\"\"\n",
    "        Every layer that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward():\n",
    "        \"\"\"\n",
    "        Every layer that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Layer.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input layer has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_layers:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Layer) constructor. Weights and bias\n",
    "        # are treated like inbound layers.\n",
    "        Layer.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_layers[0].value\n",
    "        W = self.inbound_layers[1].value\n",
    "        b = self.inbound_layers[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_layers.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_layers:\n",
    "            # Get the partial of the cost with respect to this layer.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this layer's inputs.\n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, self.inbound_layers[1].value.T)\n",
    "            # Set the partial of the loss with respect to this layer's weights.\n",
    "            self.gradients[self.inbound_layers[1]] += np.dot(self.inbound_layers[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this layer's bias.\n",
    "            self.gradients[self.inbound_layers[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    Represents a layer that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer):\n",
    "        # The base class constructor.\n",
    "        Layer.__init__(self, [layer])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_layers[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_layers[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Layer):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last layer for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Layer.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_layers[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_layers[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_layers[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_layers[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_layers[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the layers in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Layer and the value is the respective value feed to that Layer.\n",
    "\n",
    "    Returns a list of sorted layers.\n",
    "    \"\"\"\n",
    "\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    layers = [n for n in input_layers]\n",
    "    while len(layers) > 0:\n",
    "        n = layers.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Layers.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Layers representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 118.103\n",
      "Epoch: 2, Loss: 41.600\n",
      "Epoch: 3, Loss: 23.512\n",
      "Epoch: 4, Loss: 37.447\n",
      "Epoch: 5, Loss: 19.025\n",
      "Epoch: 6, Loss: 18.008\n",
      "Epoch: 7, Loss: 21.718\n",
      "Epoch: 8, Loss: 20.954\n",
      "Epoch: 9, Loss: 17.017\n",
      "Epoch: 10, Loss: 18.009\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Have fun with the number of epochs!\n",
    "\n",
    "Be warned that if you increase them too much,\n",
    "the VM will time out :)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00, ...,\n",
       "          1.53000000e+01,   3.96900000e+02,   4.98000000e+00],\n",
       "       [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
       "          1.78000000e+01,   3.96900000e+02,   9.14000000e+00],\n",
       "       [  2.72900000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
       "          1.78000000e+01,   3.92830000e+02,   4.03000000e+00],\n",
       "       ..., \n",
       "       [  6.07600000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "          2.10000000e+01,   3.96900000e+02,   5.64000000e+00],\n",
       "       [  1.09590000e-01,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "          2.10000000e+01,   3.93450000e+02,   6.48000000e+00],\n",
       "       [  4.74100000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "          2.10000000e+01,   3.96900000e+02,   7.88000000e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "        22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "        23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "        50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "        29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "        23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "        29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "        43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "        21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "        35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "        22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "        20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "        33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "        16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "        25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "        22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "        16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "        24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "        25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "        50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "        13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "         7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "        12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "         5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "         7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "        13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "        16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "        17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "        13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "        20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "        19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "        12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "        19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "        22. ,  11.9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
